!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
! MODULE PARALLEL_MODULE
!
! This module provides routines for parallelizing.
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
module parallel_module

   use module_dm, only:local_communicator,ntasks_x,ntasks_y,mytask_x,mytask_y

   integer, parameter :: HALO_WIDTH = 3
 
   integer, pointer, dimension(:,:) :: processors
   integer :: my_proc_id
  
#if defined(DM_PARALLEL) 
     include "mpif.h"
#endif

#ifdef PROMOTE_FLOAT 
#   define VARTYPE MPI_DOUBLE_PRECISION
#else      
#   define VARTYPE MPI_REAL
#endif    
 
   contains
 

   !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
   ! Name: parallel_start
   !
   ! Purpose: For MPI, the purpose of this routine is to basically set up
   !   a communicator for a rectangular mesh, and determine how many processors
   !   in the x and y directions there will be. 
   !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

   !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
   ! Name: exchange_halo_r
   !
   ! Purpose: 
   !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
   subroutine exchange_halo_r(patch_array, &
                              ms1, me1, ms2, me2, ms3, me3, &
                              ps1, pe1, ps2, pe2, ps3, pe3)
 
      implicit none

      ! Arguments
      integer, intent(in) :: ps1, pe1, ps2, pe2, ps3, pe3, &
                             ms1, me1, ms2, me2, ms3, me3
      real, dimension(ms1:me1,ms2:me2,ms3:me3), intent(inout) :: patch_array

      ! Local variables
#ifdef DM_PARALLEL
      integer :: ii,jj, kk,m
      integer :: mpi_ierr
      integer, dimension(MPI_STATUS_SIZE) :: mpi_stat
      real,dimension(:),allocatable::send_array,receive_array
      
      !
      ! Get left edge of halo
      !
      if (mytask_x /= (ntasks_x - 1)) then
      	 m=(pe3-ps3+1)*(me2-ms2+1)*HALO_WIDTH     
      	 allocate(send_array(m))
      	 m=0
         do kk=ps3,pe3
           do jj=ms2,me2
              do ii=pe1-HALO_WIDTH+1,pe1   
               m=m+1
               send_array(m)=patch_array(ii,jj,kk)
              enddo
           enddo
         enddo  
         call MPI_Send(send_array, m, VARTYPE, processors(mytask_y,mytask_x+1), my_proc_id, local_communicator, mpi_ierr)
         deallocate(send_array)    
      end if
      if (mytask_x /= 0) then
      	 m=(pe3-ps3+1)*(me2-ms2+1)*HALO_WIDTH     
      	 allocate(receive_array(m))         
         call MPI_Recv(receive_array, m, VARTYPE, &
                             processors(mytask_y,mytask_x-1), MPI_ANY_TAG, local_communicator, mpi_stat, mpi_ierr)
         m=0
         do kk=ps3,pe3
           do jj=ms2,me2
              do ii=ms1,ms1+HALO_WIDTH-1   
               m=m+1
               patch_array(ii,jj,kk)=receive_array(m)
              enddo
           enddo
         enddo  
         deallocate(receive_array)
      end if

      call mpi_barrier(local_communicator, mpi_ierr)
      !
      ! Get right edge of halo
      !
      if (mytask_x /= 0) then
      	 m=(pe3-ps3+1)*(me2-ms2+1)*HALO_WIDTH     
      	 allocate(send_array(m))
      	 m=0
         do kk=ps3,pe3
           do jj=ms2,me2
              do ii=ps1,ps1+HALO_WIDTH-1
               m=m+1
               send_array(m)=patch_array(ii,jj,kk)
              enddo
           enddo
         enddo        	      
         call MPI_Send(send_array, m, VARTYPE, processors(mytask_y,mytask_x-1), my_proc_id, local_communicator, mpi_ierr)
         deallocate(send_array)      
      end if
      if (mytask_x /= (ntasks_x - 1)) then
      	 m=(pe3-ps3+1)*(me2-ms2+1)*HALO_WIDTH     
      	 allocate(receive_array(m))
      	 call MPI_Recv(receive_array, m, VARTYPE, processors(mytask_y,mytask_x+1), MPI_ANY_TAG, local_communicator, mpi_stat, mpi_ierr)
         m=0
         do kk=ps3,pe3
           do jj=ms2,me2
              do ii=me1-HALO_WIDTH+1,me1
               m=m+1
               patch_array(ii,jj,kk)=receive_array(m)
              enddo
           enddo
         enddo  
         deallocate(receive_array)         
      end if

      call mpi_barrier(local_communicator, mpi_ierr)
      !
      ! Get bottom edge of halo
      !
      if (mytask_y /= (ntasks_y - 1)) then
      	 m=(pe3-ps3+1)*(me1-ms1+1)*HALO_WIDTH     
      	 allocate(send_array(m))
      	 m=0
         do kk=ps3,pe3
           do jj=pe2-HALO_WIDTH+1,pe2
              do ii=ms1,me1   
               m=m+1
               send_array(m)=patch_array(ii,jj,kk)
              enddo
           enddo
         enddo         	      
         call MPI_Send(send_array, m, VARTYPE, processors(mytask_y+1,mytask_x), my_proc_id, local_communicator, mpi_ierr)
         deallocate(send_array)
      end if
      if (mytask_y /= 0) then
      	 m=(pe3-ps3+1)*(me1-ms1+1)*HALO_WIDTH     
      	 allocate(receive_array(m))
      	 call MPI_Recv(receive_array, m, VARTYPE, processors(mytask_y-1,mytask_x), MPI_ANY_TAG, local_communicator, mpi_stat, mpi_ierr)
         m=0
         do kk=ps3,pe3
           do jj=ms2,ms2+HALO_WIDTH-1
              do ii=ms1,me1
               m=m+1
               patch_array(ii,jj,kk)=receive_array(m)
              enddo
           enddo
         enddo  
         deallocate(receive_array)                                   
      end if

      call mpi_barrier(local_communicator, mpi_ierr)
      !
      ! Get top edge of halo
      !
      if (mytask_y /= 0) then
      	 m=(pe3-ps3+1)*(me1-ms1+1)*HALO_WIDTH     
      	 allocate(send_array(m))
      	 m=0
         do kk=ps3,pe3
           do jj=ps2,ps2+HALO_WIDTH-1
              do ii=ms1,me1   
               m=m+1
               send_array(m)=patch_array(ii,jj,kk)
              enddo
           enddo
         enddo         	      
         call MPI_Send(send_array, m, VARTYPE, processors(mytask_y-1,mytask_x), my_proc_id, local_communicator, mpi_ierr)
         deallocate(send_array)
      end if
      if (mytask_y /= (ntasks_y - 1)) then
      	 m=(pe3-ps3+1)*(me1-ms1+1)*HALO_WIDTH     
      	 allocate(receive_array(m))
         call MPI_Recv(receive_array, m, VARTYPE, processors(mytask_y+1,mytask_x), MPI_ANY_TAG, local_communicator, mpi_stat, mpi_ierr)
         m=0
         do kk=ps3,pe3
           do jj=me2-HALO_WIDTH+1,me2
              do ii=ms1,me1
               m=m+1
               patch_array(ii,jj,kk)=receive_array(m)
              enddo
           enddo
         enddo  
         deallocate(receive_array)     
      end if

      call mpi_barrier(local_communicator, mpi_ierr)
      !
      ! Get lower-right corner of halo
      !
      if (mytask_y /= (ntasks_y - 1) .and. mytask_x /= 0) then
      	 m=(pe3-ps3+1)*HALO_WIDTH*HALO_WIDTH     
      	 allocate(send_array(m))
      	 m=0
         do kk=ps3,pe3
           do jj=pe2-HALO_WIDTH+1,pe2
              do ii=ps1,ps1+HALO_WIDTH-1
               m=m+1
               send_array(m)=patch_array(ii,jj,kk)
              enddo
           enddo
         enddo              	      
         call MPI_Send(send_array, m, VARTYPE, processors(mytask_y+1,mytask_x-1), my_proc_id, local_communicator, mpi_ierr)
         deallocate(send_array)
      end if
      if (mytask_y /= 0 .and. mytask_x /= (ntasks_x - 1)) then
      	 m=(pe3-ps3+1)*HALO_WIDTH*HALO_WIDTH     
      	 allocate(receive_array(m))      	      
         call MPI_Recv(receive_array,m, VARTYPE, processors(mytask_y-1,mytask_x+1), MPI_ANY_TAG, local_communicator, mpi_stat, mpi_ierr)
         m=0
         do kk=ps3,pe3
            do jj=ms2,ms2+HALO_WIDTH-1
              do ii=me1-HALO_WIDTH+1,me1
               m=m+1
               patch_array(ii,jj,kk)=receive_array(m)
              enddo
           enddo
         enddo  
         deallocate(receive_array)          
      end if

      call mpi_barrier(local_communicator, mpi_ierr)
      !
      ! Get upper-left corner of halo
      !
      if (mytask_y /= 0 .and. mytask_x /= (ntasks_x - 1)) then
      	 m=(pe3-ps3+1)*HALO_WIDTH*HALO_WIDTH     
      	 allocate(send_array(m))
      	 m=0
         do kk=ps3,pe3
            do jj=ps2,ps2+HALO_WIDTH-1
              do ii=pe1-HALO_WIDTH+1,pe1
               m=m+1
               send_array(m)=patch_array(ii,jj,kk)
              enddo
           enddo
         enddo    
         call MPI_Send(send_array, m, VARTYPE, processors(mytask_y-1,mytask_x+1), my_proc_id, local_communicator, mpi_ierr)
         deallocate(send_array)             
      end if
      if (mytask_y /= (ntasks_y - 1) .and. mytask_x /= 0) then
      	 m=(pe3-ps3+1)*HALO_WIDTH*HALO_WIDTH     
      	 allocate(receive_array(m))  
         call MPI_Recv(receive_array, m, VARTYPE, processors(mytask_y+1,mytask_x-1), MPI_ANY_TAG, local_communicator, mpi_stat, mpi_ierr)
         m=0
         do kk=ps3,pe3
            do jj=me2-HALO_WIDTH+1,me2
              do ii=ms1,ms1+HALO_WIDTH-1
               m=m+1
               patch_array(ii,jj,kk)=receive_array(m)
              enddo
           enddo
         enddo  
         deallocate(receive_array)   
      end if

      call mpi_barrier(local_communicator, mpi_ierr)
      !
      ! Get upper-right corner of halo
      !
      if (mytask_y /= 0 .and. mytask_x /= 0) then
       	 m=(pe3-ps3+1)*HALO_WIDTH*HALO_WIDTH     
      	 allocate(send_array(m))
      	 m=0
         do kk=ps3,pe3
            do jj=ps2,ps2+HALO_WIDTH-1
              do ii=ps1,ps1+HALO_WIDTH-1
               m=m+1
               send_array(m)=patch_array(ii,jj,kk)
              enddo
           enddo
         enddo         	
         call MPI_Send(send_array, m, VARTYPE, processors(mytask_y-1,mytask_x-1), my_proc_id, local_communicator, mpi_ierr)
         deallocate(send_array)
      end if
      if (mytask_y /= (ntasks_y - 1) .and. mytask_x /= (ntasks_x - 1)) then
      	 m=(pe3-ps3+1)*HALO_WIDTH*HALO_WIDTH     
      	 allocate(receive_array(m))         	      
         call MPI_Recv(receive_array, m, VARTYPE, processors(mytask_y+1,mytask_x+1), MPI_ANY_TAG, local_communicator, mpi_stat, mpi_ierr)
         m=0
         do kk=ps3,pe3
            do jj=me2-HALO_WIDTH+1,me2
              do ii=me1-HALO_WIDTH+1,me1
               m=m+1
               patch_array(ii,jj,kk)=receive_array(m)
              enddo
           enddo
         enddo  
         deallocate(receive_array)   
      end if

      call mpi_barrier(local_communicator, mpi_ierr)
      !
      ! Get lower-left corner of halo
      !
      if (mytask_y /= (ntasks_y - 1) .and. mytask_x /= (ntasks_x - 1)) then
       	 m=(pe3-ps3+1)*HALO_WIDTH*HALO_WIDTH     
      	 allocate(send_array(m))
      	 m=0
         do kk=ps3,pe3
            do jj=pe2-HALO_WIDTH+1,pe2
              do ii=pe1-HALO_WIDTH+1,pe1
               m=m+1
               send_array(m)=patch_array(ii,jj,kk)
              enddo
           enddo
         enddo         	
        call MPI_Send(send_array, m, VARTYPE, processors(mytask_y+1,mytask_x+1), my_proc_id, local_communicator, mpi_ierr)
        deallocate(send_array)
      end if
      if (mytask_y /= 0 .and. mytask_x /= 0) then
      	 m=(pe3-ps3+1)*HALO_WIDTH*HALO_WIDTH     
      	 allocate(receive_array(m)) 
         call MPI_Recv(receive_array, m, VARTYPE, processors(mytask_y-1,mytask_x-1), MPI_ANY_TAG, local_communicator, mpi_stat, mpi_ierr)
         m=0
         do kk=ps3,pe3
            do jj=ms2,ms2+HALO_WIDTH-1
              do ii=ms1,ms1+HALO_WIDTH-1
               m=m+1
               patch_array(ii,jj,kk)=receive_array(m)
              enddo
           enddo
         enddo  
         deallocate(receive_array)  
      end if

      call mpi_barrier(local_communicator, mpi_ierr)
#endif
  
   end subroutine exchange_halo_r

!------------------------------------------------------------------------------   

   subroutine gather_whole_field_r(patch_array, ps1, pe1, ps2, pe2, ps3, pe3, &
                                   domain_array, ds1, de1, ds2, de2, ds3, de3)
 
      implicit none
      ! Arguments
      integer, intent(in) :: ps1, pe1, ps2, pe2, ps3, pe3, &
                             ds1, de1, ds2, de2, ds3, de3
      real, dimension(ps1:pe1,ps2:pe2,ps3:pe3), intent(in) :: patch_array
      real, dimension(ds1:de1,ds2:de2,ds3:de3), intent(inout) :: domain_array
  
      ! Local variables
      integer :: i, ii, j, jj, k, kk, m
      integer, dimension(2) :: idims, jdims
      integer :: mpi_ierr
      integer, dimension(MPI_STATUS_SIZE) :: mpi_stat
      real, dimension(:),allocatable ::  av
      
#ifdef DM_PARALLEL
      if (my_proc_id .eq. 0) then
  
          do i=0,ntasks_y-1
            do j=0,ntasks_x-1
               if (processors(i,j) .ne. 0) then

                  call MPI_Recv(jdims, 2, MPI_INTEGER, processors(i,j),MPI_ANY_TAG, local_communicator, mpi_stat, mpi_ierr)
                  call MPI_Recv(idims, 2, MPI_INTEGER, processors(i,j),MPI_ANY_TAG, local_communicator, mpi_stat, mpi_ierr)

                  allocate(av((idims(2)-idims(1)+1)*(jdims(2)-jdims(1)+1)*(de2-ds2+1))) 
                  call MPI_Recv(av, (idims(2)-idims(1)+1)*(jdims(2)-jdims(1)+1)*(de2-ds2+1), &
                                 VARTYPE, processors(i,j), MPI_ANY_TAG, local_communicator, mpi_stat, mpi_ierr)

                  m=0
                  do jj=jdims(1), jdims(2)
                    do kk=ds2, de2
                      do ii=idims(1), idims(2)
                        m=m+1
                        domain_array(ii,kk,jj)=av(m)
                       enddo
                    enddo
                  enddo
                  deallocate(av)

               else
                  domain_array(ps1:pe1,ps2:pe2,ps3:pe3) = patch_array(ps1:pe1,ps2:pe2,ps3:pe3)
               end if
            end do
           end do
  
      else
  
         jdims(1) = ps3
         jdims(2) = pe3
         call MPI_Send(jdims, 2, MPI_INTEGER, 0, my_proc_id, local_communicator,mpi_ierr)
         idims(1) = ps1
         idims(2) = pe1
         call MPI_Send(idims, 2, MPI_INTEGER, 0, my_proc_id, local_communicator,mpi_ierr)

         allocate(av((pe1-ps1+1)*(pe2-ps2+1)*(pe3-ps3+1)))
         m=0
         do jj=ps3, pe3
          do kk=ps2, pe2
           do ii=ps1, pe1 
             m = m+1
             av(m)= patch_array(ii,kk,jj)
           enddo
          enddo
         enddo
         call MPI_Send(av, (pe1-ps1+1)*(pe2-ps2+1)*(pe3-ps3+1), &
                       VARTYPE, 0, my_proc_id, local_communicator, mpi_ierr)
         deallocate(av)
      end if
#endif
 
   end subroutine gather_whole_field_r

!------------------------------------------------------------------------------   

   subroutine gather_whole_field_r2(patch_array, ps1, pe1, ps2, pe2, ps3, pe3, &
                                   domain_array, ds1, de1, ds2, de2, ds3, de3)
 
      implicit none
      ! Arguments
      integer, intent(in) :: ps1, pe1, ps2, pe2, ps3, pe3, &
                             ds1, de1, ds2, de2, ds3, de3
      real, dimension(ps1:pe1,ps2:pe2,ps3:pe3,1:2), intent(in) :: patch_array
      real, dimension(ds1:de1,ds2:de2,ds3:de3,1:2), intent(inout) :: domain_array
  
      ! Local variables
      integer :: i, ii, j, jj, k, kk, m, ll  ! ll added by Wang Yuzhu
      integer, dimension(2) :: idims, jdims
      integer :: mpi_ierr
      integer, dimension(MPI_STATUS_SIZE) :: mpi_stat
      real, dimension(:),allocatable ::  av
      
#ifdef DM_PARALLEL
      if (my_proc_id .eq. 0) then
  
          do i=0,ntasks_y-1
            do j=0,ntasks_x-1
               if (processors(i,j) .ne. 0) then

                  call MPI_Recv(jdims, 2, MPI_INTEGER, processors(i,j),MPI_ANY_TAG, local_communicator, mpi_stat, mpi_ierr)
                  call MPI_Recv(idims, 2, MPI_INTEGER, processors(i,j),MPI_ANY_TAG, local_communicator, mpi_stat, mpi_ierr)

                  allocate(av((idims(2)-idims(1)+1)*(jdims(2)-jdims(1)+1)*(de2-ds2+1)*2)) 
                  call MPI_Recv(av, (idims(2)-idims(1)+1)*(jdims(2)-jdims(1)+1)*(de2-ds2+1)*2, &
                                 VARTYPE, processors(i,j), MPI_ANY_TAG, local_communicator, mpi_stat, mpi_ierr)

                  m=0
                  do ll=1,2
                  do jj=jdims(1), jdims(2)
                    do kk=ds2, de2
                      do ii=idims(1), idims(2)
                        m=m+1
                        domain_array(ii,kk,jj,ll)=av(m)
                       enddo
                    enddo
                  enddo
                  deallocate(av)

               else
                  domain_array(ps1:pe1,ps2:pe2,ps3:pe3,1:2) = patch_array(ps1:pe1,ps2:pe2,ps3:pe3,1:2)
               end if
            end do
           end do
  
      else
  
         jdims(1) = ps3
         jdims(2) = pe3
         call MPI_Send(jdims, 2, MPI_INTEGER, 0, my_proc_id, local_communicator,mpi_ierr)
         idims(1) = ps1
         idims(2) = pe1
         call MPI_Send(idims, 2, MPI_INTEGER, 0, my_proc_id, local_communicator,mpi_ierr)

         allocate(av((pe1-ps1+1)*(pe2-ps2+1)*(pe3-ps3+1)*2))
         m=0
         do ll=1,2
         do jj=ps3, pe3
          do kk=ps2, pe2
           do ii=ps1, pe1 
             m = m+1
             av(m)= patch_array(ii,kk,jj,ll)
           enddo
          enddo
         enddo
         call MPI_Send(av, (pe1-ps1+1)*(pe2-ps2+1)*(pe3-ps3+1)*2, &
                       VARTYPE, 0, my_proc_id, local_communicator, mpi_ierr)
         deallocate(av)
      end if
#endif
 
   end subroutine gather_whole_field_r2

!------------------------------------------------------------------------------   

   subroutine gather_whole_field_db(patch_array, ps1, pe1, ps2, pe2, ps3, pe3, &
                                    domain_array, ds1, de1, ds2, de2, ds3, de3)
 
      implicit none
#if defined(DM_PARALLEL)
     include "mpif.h"
#endif
      ! Arguments
      integer, intent(in) :: ps1, pe1, ps2, pe2, ps3, pe3, &
                             ds1, de1, ds2, de2, ds3, de3
      real(8), dimension(ps1:pe1,ps2:pe2,ps3:pe3), intent(in) :: patch_array
      real(8), dimension(ds1:de1,ds2:de2,ds3:de3), intent(inout) :: domain_array
  
      ! Local variables
      integer :: i, ii, j, jj, k, kk, m,tag
      integer, dimension(2) :: idims, jdims
      integer, dimension(2,ntasks_y*ntasks_x) :: fidims, fjdims
      integer :: mpi_ierr
      integer, dimension(MPI_STATUS_SIZE) :: mpi_stat
      real(8), dimension(:),allocatable ::  av
      
#ifdef DM_PARALLEL
       
      if (my_proc_id .eq. 0) then
          do i=0,ntasks_y-1
            do j=0,ntasks_x-1
               if (processors(i,j) .ne. 0) then

                  call MPI_Recv(jdims, 2, MPI_INTEGER, processors(i,j),MPI_ANY_TAG, local_communicator, mpi_stat, mpi_ierr)
                  call MPI_Recv(idims, 2, MPI_INTEGER, processors(i,j),MPI_ANY_TAG, local_communicator, mpi_stat, mpi_ierr)

                  allocate(av((idims(2)-idims(1)+1)*(jdims(2)-jdims(1)+1)*(de2-ds2+1)))
                  call MPI_Recv(av,(idims(2)-idims(1)+1)*(jdims(2)-jdims(1)+1)*(de2-ds2+1), &
                                MPI_DOUBLE_PRECISION, processors(i,j), MPI_ANY_TAG,local_communicator, mpi_stat, mpi_ierr)

                  m=0
                  do jj=jdims(1), jdims(2)
                    do kk=ds2, de2
                      do ii=idims(1), idims(2)
                        m=m+1
                        domain_array(ii,kk,jj)=av(m)
                       enddo
                    enddo
                  enddo
                  deallocate(av)

               else
                  domain_array(ps1:pe1,ps2:pe2,ps3:pe3) = patch_array(ps1:pe1,ps2:pe2,ps3:pe3)
               end if
            end do
           end do

      else
 
         jdims(1) = ps3
         jdims(2) = pe3
         call MPI_Send(jdims, 2, MPI_INTEGER, 0, my_proc_id, local_communicator,mpi_ierr)
         idims(1) = ps1
         idims(2) = pe1
         call MPI_Send(idims, 2, MPI_INTEGER, 0, my_proc_id, local_communicator,mpi_ierr)
 
         allocate(av((pe1-ps1+1)*(pe2-ps2+1)*(pe3-ps3+1)))
         m=0
         do jj=ps3, pe3
          do kk=ps2, pe2
           do ii=ps1, pe1 
             m = m+1
             av(m)= patch_array(ii,kk,jj)
           enddo
          enddo
         enddo
         call MPI_Send(av, (pe1-ps1+1)*(pe2-ps2+1)*(pe3-ps3+1), &
                       MPI_DOUBLE_PRECISION, 0, my_proc_id, local_communicator, mpi_ierr)
         deallocate(av)
      end if
#endif
 
   end subroutine gather_whole_field_db
!------------------------------------------------------------------------------   
   subroutine exchange_xbdy_halo_r(patch_array, spec_bdy_width, &
                              ms1, me1, ms2, me2, ms3, me3, &
                              ps1, pe1, ps2, pe2, ps3, pe3, &
                              ds1, de1, ds2, de2, ds3, de3)
 
      implicit none

      ! Arguments
      integer, intent(in) :: ps1, pe1, ps2, pe2, ps3, pe3, &
                              ms1, me1, ms2, me2, ms3, me3, &
                              ds1, de1, ds2, de2, ds3, de3
      integer, intent(in) :: spec_bdy_width                               
      real, dimension(ms1:me1,ms3:me3,spec_bdy_width), intent(inout) :: patch_array

      ! Local variables
#ifdef DM_PARALLEL
      integer :: ii,jj, kk,m
      integer :: mpi_ierr
      integer, dimension(MPI_STATUS_SIZE) :: mpi_stat
      real,dimension(:),allocatable::send_array,receive_array
      
      !
      ! Y-start Boundary, Get left edge of halo
      !
      if((ps2-ds2).lt.spec_bdy_width) then
      
      if (mytask_x /= (ntasks_x - 1)) then
      	 m=(pe3-ps3+1)*spec_bdy_width*HALO_WIDTH     
      	 allocate(send_array(m))
      	 m=0
         do kk=ps3,pe3
           do jj=1,spec_bdy_width
              do ii=pe1-HALO_WIDTH+1,pe1   
               m=m+1
               send_array(m)=patch_array(ii,kk,jj)
              enddo
           enddo
         enddo  
         call MPI_Send(send_array, m, VARTYPE, processors(mytask_y,mytask_x+1), my_proc_id, local_communicator, mpi_ierr)
         deallocate(send_array)    
      end if
      if (mytask_x /= 0) then
      	 m=(pe3-ps3+1)*spec_bdy_width*HALO_WIDTH     
      	 allocate(receive_array(m))         
         call MPI_Recv(receive_array, m, VARTYPE, &
                             processors(mytask_y,mytask_x-1), MPI_ANY_TAG, local_communicator, mpi_stat, mpi_ierr)
         m=0
         do kk=ps3,pe3
           do jj=1,spec_bdy_width
              do ii=ps1-HALO_WIDTH,ps1-1   
               m=m+1
               patch_array(ii,kk,jj)=receive_array(m)
              enddo
           enddo
         enddo  
         deallocate(receive_array)
      end if

      end if

      call mpi_barrier(local_communicator, mpi_ierr)

      !
      ! Y-start Boundary, Get right edge of halo
      !
      if((ps2-ds2).lt.spec_bdy_width) then

      if (mytask_x /= 0) then
      	 m=(pe3-ps3+1)*spec_bdy_width*HALO_WIDTH     
      	 allocate(send_array(m))
      	 m=0
         do kk=ps3,pe3
           do jj=1,spec_bdy_width
              do ii=ps1,ps1+HALO_WIDTH-1
               m=m+1
               send_array(m)=patch_array(ii,kk,jj)
              enddo
           enddo
         enddo        	      
         call MPI_Send(send_array, m, VARTYPE, processors(mytask_y,mytask_x-1), my_proc_id, local_communicator, mpi_ierr)
         deallocate(send_array)      
      end if
      if (mytask_x /= (ntasks_x - 1)) then
      	 m=(pe3-ps3+1)*spec_bdy_width*HALO_WIDTH     
      	 allocate(receive_array(m))
      	 call MPI_Recv(receive_array, m, VARTYPE, processors(mytask_y,mytask_x+1), MPI_ANY_TAG, local_communicator, mpi_stat, mpi_ierr)
         m=0
         do kk=ps3,pe3
           do jj=1,spec_bdy_width
              do ii=pe1+1,pe1+HALO_WIDTH
               m=m+1
               patch_array(ii,kk,jj)=receive_array(m)
              enddo
           enddo
         enddo  
         deallocate(receive_array)         
      end if

      end if

      call mpi_barrier(local_communicator, mpi_ierr)
      
      !
      ! Y-end Boundary, Get left edge of halo
      !
      if((de2-pe2).lt.spec_bdy_width) then
      
      if (mytask_x /= (ntasks_x - 1)) then
      	 m=(pe3-ps3+1)*spec_bdy_width*HALO_WIDTH     
      	 allocate(send_array(m))
      	 m=0
         do kk=ps3,pe3
           do jj=1,spec_bdy_width
              do ii=pe1-HALO_WIDTH+1,pe1   
               m=m+1
               send_array(m)=patch_array(ii,kk,jj)
              enddo
           enddo
         enddo  
         call MPI_Send(send_array, m, VARTYPE, processors(mytask_y,mytask_x+1), my_proc_id, local_communicator, mpi_ierr)
         deallocate(send_array)    
      end if
      if (mytask_x /= 0) then
      	 m=(pe3-ps3+1)*spec_bdy_width*HALO_WIDTH     
      	 allocate(receive_array(m))         
         call MPI_Recv(receive_array, m, VARTYPE, &
                             processors(mytask_y,mytask_x-1), MPI_ANY_TAG, local_communicator, mpi_stat, mpi_ierr)
         m=0
         do kk=ps3,pe3
           do jj=1,spec_bdy_width
              do ii=ps1-HALO_WIDTH,ps1-1   
               m=m+1
               patch_array(ii,kk,jj)=receive_array(m)
              enddo
           enddo
         enddo  
         deallocate(receive_array)
      end if
     
      end if

      call mpi_barrier(local_communicator, mpi_ierr)

      !
      ! Y-end Boundary, Get right edge of halo
      !
      if((de2-pe2).lt.spec_bdy_width) then

      if (mytask_x /= 0) then
      	 m=(pe3-ps3+1)*spec_bdy_width*HALO_WIDTH     
      	 allocate(send_array(m))
      	 m=0
         do kk=ps3,pe3
           do jj=1,spec_bdy_width
              do ii=ps1,ps1+HALO_WIDTH-1
               m=m+1
               send_array(m)=patch_array(ii,kk,jj)
              enddo
           enddo
         enddo        	      
         call MPI_Send(send_array, m, VARTYPE, processors(mytask_y,mytask_x-1), my_proc_id, local_communicator, mpi_ierr)
         deallocate(send_array)      
      end if
      if (mytask_x /= (ntasks_x - 1)) then
      	 m=(pe3-ps3+1)*spec_bdy_width*HALO_WIDTH     
      	 allocate(receive_array(m))
      	 call MPI_Recv(receive_array, m, VARTYPE, processors(mytask_y,mytask_x+1), MPI_ANY_TAG, local_communicator, mpi_stat, mpi_ierr)
         m=0
         do kk=ps3,pe3
           do jj=1,spec_bdy_width
              do ii=pe1+1,pe1+HALO_WIDTH
               m=m+1
               patch_array(ii,kk,jj)=receive_array(m)
              enddo
           enddo
         enddo  
         deallocate(receive_array)         
      end if

      end if

      call mpi_barrier(local_communicator, mpi_ierr)
      
#endif
  
   end subroutine exchange_xbdy_halo_r
   
!------------------------------------------------------------------------------   
   subroutine exchange_ybdy_halo_r(patch_array, spec_bdy_width, &
                              ms1, me1, ms2, me2, ms3, me3, &
                              ps1, pe1, ps2, pe2, ps3, pe3, &
                              ds1, de1, ds2, de2, ds3, de3)
 
      implicit none

      ! Arguments
      integer, intent(in) :: ps1, pe1, ps2, pe2, ps3, pe3, &
                              ms1, me1, ms2, me2, ms3, me3, &
                              ds1, de1, ds2, de2, ds3, de3
      integer, intent(in) :: spec_bdy_width                               
      real, dimension(ms2:me2,ms3:me3,spec_bdy_width), intent(inout) :: patch_array

      ! Local variables
#ifdef DM_PARALLEL
      integer :: ii,jj, kk,m
      integer :: mpi_ierr
      integer, dimension(MPI_STATUS_SIZE) :: mpi_stat
      real,dimension(:),allocatable::send_array,receive_array
      
      !
      ! x-start boundary, Get bottom edge of halo
      !
      if((ps1-ds1).lt.spec_bdy_width) then

      if (mytask_y /= (ntasks_y - 1)) then
      	 m=(pe3-ps3+1)*spec_bdy_width*HALO_WIDTH     
      	 allocate(send_array(m))
      	 m=0
         do kk=ps3,pe3
           do jj=pe2-HALO_WIDTH+1,pe2
              do ii=1,spec_bdy_width   
               m=m+1
               send_array(m)=patch_array(jj,kk,ii)
              enddo
           enddo
         enddo         	      
         call MPI_Send(send_array, m, VARTYPE, processors(mytask_y+1,mytask_x), my_proc_id, local_communicator, mpi_ierr)
         deallocate(send_array)
      end if
      if (mytask_y /= 0) then
      	 m=(pe3-ps3+1)*spec_bdy_width*HALO_WIDTH     
      	 allocate(receive_array(m))
      	 call MPI_Recv(receive_array, m, VARTYPE, processors(mytask_y-1,mytask_x), MPI_ANY_TAG, local_communicator, mpi_stat, mpi_ierr)
         m=0
         do kk=ps3,pe3
           do jj=ps2-HALO_WIDTH,ps2-1
              do ii=1,spec_bdy_width
               m=m+1
               patch_array(jj,kk,ii)=receive_array(m)
              enddo
           enddo
         enddo  
         deallocate(receive_array)                                   
      end if

      end if

      call mpi_barrier(local_communicator, mpi_ierr)

      !
      ! x-start boundary, Get top edge of halo
      !
      if((ps1-ds1).lt.spec_bdy_width) then

      if (mytask_y /= 0) then
      	 m=(pe3-ps3+1)*spec_bdy_width*HALO_WIDTH     
      	 allocate(send_array(m))
      	 m=0
         do kk=ps3,pe3
           do jj=ps2,ps2+HALO_WIDTH-1
              do ii=1,spec_bdy_width
               m=m+1
               send_array(m)=patch_array(jj,kk,ii)
              enddo
           enddo
         enddo         	      
         call MPI_Send(send_array, m, VARTYPE, processors(mytask_y-1,mytask_x), my_proc_id, local_communicator, mpi_ierr)
         deallocate(send_array)
      end if
      if (mytask_y /= (ntasks_y - 1)) then
      	 m=(pe3-ps3+1)*spec_bdy_width*HALO_WIDTH     
      	 allocate(receive_array(m))
         call MPI_Recv(receive_array, m, VARTYPE, processors(mytask_y+1,mytask_x), MPI_ANY_TAG, local_communicator, mpi_stat, mpi_ierr)
         m=0
         do kk=ps3,pe3
           do jj=pe2+1,pe2+HALO_WIDTH
              do ii=1,spec_bdy_width
               m=m+1
               patch_array(jj,kk,ii)=receive_array(m)
              enddo
           enddo
         enddo  
         deallocate(receive_array)     
      end if

      end if

      call mpi_barrier(local_communicator, mpi_ierr)
      
      !
      ! x-end boundary, Get bottom edge of halo
      !      	      
      if((de1-pe1).lt.spec_bdy_width) then

      if (mytask_y /= (ntasks_y - 1)) then
      	 m=(pe3-ps3+1)*spec_bdy_width*HALO_WIDTH     
      	 allocate(send_array(m))
      	 m=0
         do kk=ps3,pe3
           do jj=pe2-HALO_WIDTH+1,pe2
              do ii=1,spec_bdy_width 
               m=m+1
               send_array(m)=patch_array(jj,kk,ii)
              enddo
           enddo
         enddo         	      
         call MPI_Send(send_array, m, VARTYPE, processors(mytask_y+1,mytask_x), my_proc_id, local_communicator, mpi_ierr)
         deallocate(send_array)
      end if
      if (mytask_y /= 0) then
      	 m=(pe3-ps3+1)*spec_bdy_width*HALO_WIDTH     
      	 allocate(receive_array(m))
      	 call MPI_Recv(receive_array, m, VARTYPE, processors(mytask_y-1,mytask_x), MPI_ANY_TAG, local_communicator, mpi_stat, mpi_ierr)
         m=0
         do kk=ps3,pe3
           do jj=ps2-HALO_WIDTH,ps2-1
              do ii=1,spec_bdy_width
               m=m+1
               patch_array(jj,kk,ii)=receive_array(m)
              enddo
           enddo
         enddo  
         deallocate(receive_array)                                   
      end if

      end if

      call mpi_barrier(local_communicator, mpi_ierr)

      !
      ! x-end boundary, Get top edge of halo
      !
      if((de1-pe1).lt.spec_bdy_width) then

      if (mytask_y /= 0) then
      	 m=(pe3-ps3+1)*spec_bdy_width*HALO_WIDTH     
      	 allocate(send_array(m))
      	 m=0
         do kk=ps3,pe3
           do jj=ps2,ps2+HALO_WIDTH-1
              do ii=1,spec_bdy_width
               m=m+1
               send_array(m)=patch_array(jj,kk,ii)
              enddo
           enddo
         enddo         	      
         call MPI_Send(send_array, m, VARTYPE, processors(mytask_y-1,mytask_x), my_proc_id, local_communicator, mpi_ierr)
         deallocate(send_array)
      end if
      if (mytask_y /= (ntasks_y - 1)) then
      	 m=(pe3-ps3+1)*spec_bdy_width*HALO_WIDTH     
      	 allocate(receive_array(m))
         call MPI_Recv(receive_array, m, VARTYPE, processors(mytask_y+1,mytask_x), MPI_ANY_TAG, local_communicator, mpi_stat, mpi_ierr)
         m=0
         do kk=ps3,pe3
           do jj=pe2+1,pe2+HALO_WIDTH
              do ii=1,spec_bdy_width
               m=m+1
               patch_array(jj,kk,ii)=receive_array(m)
              enddo
           enddo
         enddo  
         deallocate(receive_array)     
      end if

      end if     
 
      call mpi_barrier(local_communicator, mpi_ierr)
      
#endif
  
   end subroutine exchange_ybdy_halo_r

end module parallel_module
